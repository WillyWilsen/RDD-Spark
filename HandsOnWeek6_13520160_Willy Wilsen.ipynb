{"cells":[{"cell_type":"markdown","metadata":{"id":"0uDup6H5Nakk"},"source":["# HandsOn Week 6 RDD Spark\n","Selamat datang di HandsOn Week 6, yaitu tentang pemrosesan data terdistribusi menggunakan Spark. Untuk tujuan pembelajaran, seperti biasa, kita akan menggunakan *pseudo-distributed mode* (single node cluster) di VM yang telah disediakan. Dengan kode yang *similar* di cluster komputer dengan *n* workers, maka komputasi akan tersebar ke *n* workers tersebut. Adapun yang akan kita coba kali ini adalah melakukan komputasi menggunakan RDD dan DataFrame. Berikut catatan-catatan yang perlu kamu perhatikan dalam hands-on ini:\n","1. Untuk menjalankan Apache Spark dalam bahasa python di VM, ketikkan perintah ```pyspark``` di terminal.\n","2. Dari semua Milestone, data input yang digunakan adalah data \"purchases.txt\" yang telah diletakkan di HDFS. Oleh karena itu, pastikan hadoop service kamu berjalan (```start-dfs.sh```, ```start-yarn.sh```, ```jps```). Untuk membaca data dari HDFS, lihat kembali di slide perkuliahan.\n","3. Untuk Milestone 1, 2 dan 3, kalian perlu untuk mencatat waktu yang diperlukan saat melakukan MapReduce menggunakan hadoop streaming jar di hands-on sebelumnya. Waktu bisa dihitung dari selisih \"waktu awal\" dan \"waktu akhir\" yang tampak di terminal saat kalian selesain melakukan MapReduce -atau menggunakan cara lain yang masih *acceptable*-. (lihat ilustrasi di bawah).\n","4. Lakukan zip file jupyter notebook ini beserta gambar-gambar yang diperlukan -screenshot waktu proses MapReduce Hadoop jar-, dan submit ke portal kuliah EDUNEX dengan format nama \"**HandsOnWeek10_NIM_NamaLengkap.zip**\". Pastikan file jupyter notebook yang kamu zip dalam kondisi memiliki output per cellnya (tidak kosong karena belum dijalankan). <br>\n","\n","<img title=\"Waktu Awal\" align=\"left\" src=\"waktu_awal.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n","<img title=\"Waktu Akhir\" align=\"left\" src=\"waktu_akhir.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"0HJOcG2mNaks"},"source":["## Milestone 1\n","Kerjakan Milestone 1 pada HandsOn Week 6(sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caB7L1cKNaks","outputId":"5f26548d-0307-412c-939b-5ec799f6bab2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Waktu yang diperlukan dengan Hadoop: <isikan waktunya>\n","Waktu yang diperlukan dengan RDD Spark: <gunakan library time untuk menghitung deltanya>\n"]}],"source":["from time import time\n","\n","# Tuliskan code kamu di sini\n","start_time = time()\n","RDD1 = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n","# Mapper\n","RDD2 = RDD1.map(lambda x: x.split(\"\\t\")).filter(lambda x: len(x) == 6).map(lambda x: (x[3], float(x[4])))\n","# Reducer\n","RDD3 = RDD2.filter(lambda x: \"Consumer Electronics\" in x[0] or \"Toys\" in x[0]).reduceByKey(lambda x, y: x + y)\n","print(RDD3.collect())\n","end_time = time()\n","\n","print(\"Waktu yang diperlukan dengan Hadoop:\", \"57 detik\")\n","print(\"Waktu yang diperlukan dengan RDD Spark:\", f\"{end_time - start_time} detik\")"]},{"cell_type":"markdown","metadata":{"id":"W4gk9-rwNakv"},"source":["<img title=\"Waktu Spark\" align=\"left\" src=\"spark_milestone1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n","<img title=\"Waktu Awal\" align=\"left\" src=\"waktu_awal_milestone1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n","<img title=\"Waktu Akhir\" align=\"left\" src=\"waktu_akhir_milestone1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"aEb0MYjsNakv"},"source":["## Milestone 2\n","Kerjakan Milestone 2 pada HandsOn Week5 (sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQskYTrwNakw","outputId":"05cc1653-2be4-4cf2-ef9e-db92793e75b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Waktu yang diperlukan dengan Hadoop: <isikan waktunya>\n","Waktu yang diperlukan dengan RDD Spark: <gunakan library time untuk menghitung deltanya>\n"]}],"source":["from time import time\n","\n","# Tuliskan code kamu di sini\n","start_time = time()\n","RDD1 = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n","# Mapper\n","RDD2 = RDD1.map(lambda x: x.split(\"\\t\")).filter(lambda x: len(x) == 6).map(lambda x: (x[2], (float(x[4]), x[3])))\n","# Reducer\n","RDD3 = RDD2.filter(lambda x: x[0] == \"Atlanta\" or x[0] == \"Miami\" or x[0] == \"San Francisco\").reduceByKey(lambda x, y: x if x[0] > y[0] else y)\n","print(RDD3.collect())\n","end_time = time()\n","\n","print(\"Waktu yang diperlukan dengan Hadoop:\", \"66 detik\")\n","print(\"Waktu yang diperlukan dengan RDD Spark:\", f\"{end_time - start_time} detik\")"]},{"cell_type":"markdown","metadata":{"id":"KeKZRzqCNakx"},"source":["<img title=\"Waktu Spark\" align=\"left\" src=\"spark_milestone2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n","<img title=\"Waktu Awal\" align=\"left\" src=\"waktu_awal_milestone2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n","<img title=\"Waktu Akhir\" align=\"left\" src=\"waktu_akhir_milestone2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"aurKREgYNaky"},"source":["## Milestone 3\n","Kerjakan Milestone 3 pada HandsOn Week5 (sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoWcOJtYNakz","outputId":"9be42dd8-b6b5-4f66-e415-82d468164690"},"outputs":[{"name":"stdout","output_type":"stream","text":["Waktu yang diperlukan dengan Hadoop: <isikan waktunya>\n","Waktu yang diperlukan dengan RDD Spark: <gunakan library time untuk menghitung deltanya>\n"]}],"source":["# Tuliskan code kamu di sini\n","\n","print(\"Waktu yang diperlukan dengan Hadoop:\", \"49 detik\")\n","print(\"Waktu yang diperlukan dengan RDD Spark:\", \"<gunakan library time untuk menghitung deltanya>\")"]},{"cell_type":"markdown","metadata":{"id":"KYXjDgBzNak0"},"source":["<img title=\"Waktu Spark\" align=\"left\" src=\"spark_milestone3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n","<img title=\"Waktu Awal\" align=\"left\" src=\"waktu_awal_milestone3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n","<img title=\"Waktu Akhir\" align=\"left\" src=\"waktu_akhir_milestone3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"aBWr5glANak1"},"source":["## Milestone 4\n","Milestone ini dibagi menjadi 4.1, 4.2 dan 4.3 yang masing-masing secara berturut-turut adalah mengerjakan ulang Milestone 1, 2 dan 3 di atas (menggunakan RDD Spark), akan tetapi menggunakan trik \"**persist() RDD**\" untuk mempercepat prosesnya. Kamu bisa melakukan \"**persist**\" untuk RDD mana saja yang kamu anggap dapat memberikan waktu proses tercepat."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogD4MxwoNak1","outputId":"450d6cb3-38ff-4021-9fc1-0778e5ac030c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Waktu Milestone 1: <isikan>  vs. Waktu Milestone 4.1: <isikan>\n","Waktu Milestone 2: <isikan>  vs. Waktu Milestone 4.2: <isikan>\n","Waktu Milestone 3: <isikan>  vs. Waktu Milestone 4.3: <isikan>\n"]}],"source":["from time import time\n","\n","# Tuliskan code kamu di sini\n","RDD1_persist = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\").persist()\n","RDD2_persist = RDD1_persist.map(lambda x: x.split(\"\\t\")).filter(lambda x: len(x) == 6).persist()\n","\n","# Milestone 1\n","start_time1 = time()\n","RDD1 = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n","# Mapper\n","RDD2 = RDD1.map(lambda x: x.split(\"\\t\")).filter(lambda x: len(x) == 6).map(lambda x: (x[3], float(x[4])))\n","# Reducer\n","RDD3 = RDD2.filter(lambda x: \"Consumer Electronics\" in x[0] or \"Toys\" in x[0]).reduceByKey(lambda x, y: x + y)\n","print(RDD3.collect())\n","end_time1 = time()\n","\n","# Milestone 2\n","start_time2 = time()\n","RDD1 = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n","# Mapper\n","RDD2 = RDD1.map(lambda x: x.split(\"\\t\")).filter(lambda x: len(x) == 6).map(lambda x: (x[2], (float(x[4]), x[3])))\n","# Reducer\n","RDD3 = RDD2.filter(lambda x: x[0] == \"Atlanta\" or x[0] == \"Miami\" or x[0] == \"San Francisco\").reduceByKey(lambda x, y: x if x[0] > y[0] else y)\n","print(RDD3.collect())\n","end_time2 = time()\n","\n","# Milestone 3\n","start_time3 = time()\n","RDD1 = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n","# Mapper\n","RDD2 = RDD1.map(lambda x: x.split(\"\\t\")).filter(lambda x: len(x) == 6).map(lambda x: (x[1], 1))\n","# Reducer\n","RDD3 = RDD2.map(lambda x: (\"09:01-10:00\" if (x[0] >= \"09:01\" and x[0] <= \"10:00\") else (\"10:01-11:00\" if (x[0] >= \"10:01\" and x[0] <= \"11:00\") else None), x[1])).filter(lambda x: x[0] is not None).reduceByKey(lambda x, y: x + y)\n","print(RDD3.collect())\n","end_time3 = time()\n","\n","print(\"Waktu Milestone 1:\", \"15.440208435058594 detik\", \" vs. Waktu Milestone 4.1:\", f\"{end_time1 - start_time1} detik\")\n","print(\"Waktu Milestone 2:\", \"15.236180543899536 detik\", \" vs. Waktu Milestone 4.2:\", f\"{end_time2 - start_time2} detik\")\n","print(\"Waktu Milestone 3:\", \"21.71329975128174 detik\", \" vs. Waktu Milestone 4.3:\", f\"{end_time3 - start_time3} detik\")"]},{"cell_type":"markdown","metadata":{},"source":["<img title=\"Waktu Spark\" align=\"left\" src=\"spark_milestone4.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"-in09VQdNak2"},"source":["## Milestone 5\n","Milestone ini dibagi menjadi 5.1, 5.2 dan 5.3 yang masing-masing secara berturut-turut adalah mengerjakan ulang Milestone 1, 2 dan 3 di atas, akan tetapi menggunakan DataFrame dari Apache Spark. Catat waktu yang diperlukan untuk masing-masing proses (5.1, 5.2 dan 5.3)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWw3UZWiNak2","outputId":"3963d841-253b-405f-d8d9-5506dd4a0c61"},"outputs":[{"name":"stdout","output_type":"stream","text":["Waktu Milestone 5.1: <isikan>\n","Waktu Milestone 5.2: <isikan>\n","Waktu Milestone 5.3: <isikan>\n"]}],"source":["from time import time\n","from pyspark.sql.functions import col, sum, max\n","\n","# Tuliskan code kamu di sini\n","df = spark.read.option(\"delimiter\", \"\\t\").csv(\"hdfs://localhost:9000/purchases/purchases.txt\")\n","df = df.withColumn(\"_c4\", df[\"_c4\"].cast(\"float\"))\n","\n","# Milestone 1\n","start_time1 = time()\n","result = df.filter(col(\"_c3\").isin([\"Consumer Electronics\", \"Toys\"])).groupBy(\"_c3\").agg(sum(\"_c4\").alias(\"sum_c4\")).collect()\n","result = [(row[\"_c3\"], row[\"sum_c4\"]) for row in result]\n","print(result)\n","end_time1 = time()\n","\n","# Milestone 2\n","start_time2 = time()\n","result = df.filter(col(\"_c2\").isin([\"Atlanta\", \"Miami\", \"San Francisco\"])).groupBy(\"_c2\").agg(max(\"_c4\").alias(\"max_c4\")).collect()\n","result = [(row[\"_c2\"], row[\"max_c4\"], df.filter((col(\"_c2\") == row[\"_c2\"]) & (col(\"_c4\") == row[\"max_c4\"])).select(\"_c3\").collect()[0][\"_c3\"]) for row in result]\n","print(result)\n","end_time2 = time()\n","\n","# Milestone 3\n","start_time3 = time()\n","result = df.filter((col(\"_c1\") >= \"09:01\") & (col(\"_c1\") <= \"11:00\")).withColumn(\"time_interval\", (col(\"_c1\") >= \"09:01\") & (col(\"_c1\") <= \"10:00\")).selectExpr(\"IF(time_interval, '09:01 - 10:00', '10:01 - 11:00') as time_interval\").groupBy(\"time_interval\").count().collect()\n","result = [(row[\"time_interval\"], row[\"count\"]) for row in result]\n","print(result)\n","end_time3 = time()\n","\n","print(\"Waktu Milestone 5.1:\", f\"{end_time1 - start_time1}\")\n","print(\"Waktu Milestone 5.2:\", f\"{end_time2 - start_time2}\")\n","print(\"Waktu Milestone 5.3:\", f\"{end_time3 - start_time3}\")"]},{"cell_type":"markdown","metadata":{},"source":["<img title=\"Waktu Spark\" align=\"left\" src=\"spark_milestone5.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":0}
